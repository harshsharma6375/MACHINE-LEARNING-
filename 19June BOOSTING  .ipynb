{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aa70930a-86ab-402c-b40e-720c237d37af",
   "metadata": {},
   "source": [
    "### BOOSTING "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4dd1976-8fb1-49de-b60f-110573c53aa2",
   "metadata": {},
   "source": [
    "It is a popular technique that is used in ensemble learning which combines .\n",
    "\n",
    "TYPE OF BOOSTING \n",
    "\n",
    "1)ADA BOOST-> (ADAPTIVE BOSST) it assigns higher weights to misclassified instance and focuses on the instances during sub sequent iteratiton .\n",
    "It sequently train a series of weak learner and combines their prediction to form the final ensemble  . \n",
    "\n",
    "ADA boost is used for binary classification problems .\n",
    "\n",
    "2) GRADIENT BOOST -> It builds the ensemble of weak learners in a stage wise manner each sub sequent model is trained to  correct the mistakes made by the previous models by fitting the negative gradient of a loss function .\n",
    "\n",
    "    #### TYPES OF GRADIENT BOSSTING \n",
    "\n",
    "A) X G boost (Extreme gradient boosting ) -->  It is an optimized implimentaion of gradient boosting \n",
    "that offers several enhancement inculding parallel processing , Regularization , technique and handling missing values . \n",
    "it uses a combination of tree based models and linear models for boosting which allows it to capture both linear and non linear relationships in the data efficiently .\n",
    "\n",
    "\n",
    "B) Light GBM --> This framework focuses on achieving faster training speed and lower memory uses it uses on novel tree growing algorithm that is called gradient based 1 side sampling to select the most informative instances for building decision trees . \n",
    "\n",
    "\n",
    "C) CAT boost --> It is designed to handle categorical features directly without the need for extensive data preprocessing it incorporates an innovation methods to handle categorical variables which includes  apply a combination of ordered boosting random permutations and symmetric trees . "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "de358e6d-884b-4b02-bb71-c765dde48c56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: xgboost in c:\\users\\hp\\anaconda3\\lib\\site-packages (3.0.2)\n",
      "Requirement already satisfied: numpy in c:\\users\\hp\\anaconda3\\lib\\site-packages (from xgboost) (1.26.4)\n",
      "Requirement already satisfied: scipy in c:\\users\\hp\\anaconda3\\lib\\site-packages (from xgboost) (1.13.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ec120432-5270-4123-aa36-68523cf1e571",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\anaconda3\\Lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adaboost Accuracy: 0.85\n",
      "xgboost Accuracy: 0.895\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# generating a synthetic classification dataset\n",
    "# Generating a synthetic classification dataset\n",
    "X, y = make_classification(n_samples=1000, n_features=10, random_state=42)\n",
    "\n",
    "# Splitting the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Creating and training an AdaBoost classifier\n",
    "adaboost = AdaBoostClassifier(n_estimators=100, random_state=42)\n",
    "adaboost.fit(X_train, y_train)\n",
    "\n",
    "# Creating and training an XGBoost classifier\n",
    "xgboost = XGBClassifier(n_estimators=100, random_state=42)\n",
    "xgboost.fit(X_train, y_train)\n",
    "\n",
    "# Making predictions on the test set for AdaBoost\n",
    "y_pred_adaboost = adaboost.predict(X_test)\n",
    "\n",
    "# Making predictions on the test set for XGBoost\n",
    "y_pred_xgboost = xgboost.predict(X_test)\n",
    "\n",
    "# Calculating the accuracy of AdaBoost\n",
    "accuracy_adaboost = accuracy_score(y_test, y_pred_adaboost)\n",
    "print(\"Adaboost Accuracy:\" , accuracy_adaboost)\n",
    "\n",
    "# calculating the accuracy of xgboost\n",
    "accuracy_xgboost = accuracy_score(y_test , y_pred_xgboost)\n",
    "print(\"xgboost Accuracy:\" , accuracy_xgboost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd4cafda-4c23-4c9c-a7e4-1fec7afd00da",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
